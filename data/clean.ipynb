{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82f86c-85f9-4ada-a395-ba7c17115a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "stopwords = stopwords.words('english')\n",
    "tqdm.pandas()\n",
    "from english_words import english_words_lower_alpha_set as word_list\n",
    "from textblob import TextBlob\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d9ac1-d0ac-443b-bc91-7f032bc2add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c1ffa-7a80-41e7-bbe2-e2787c1a76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be68290-e756-4772-99a5-9e8399d3b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {\"can't\":\"cannot\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2abc74-904c-40ad-89de-bab5e39daa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3e9ea-6ca2-4c1a-9588-bf6b63836dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports and global variables related to lemmatization\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "\n",
    "tree_tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tree_tag_map['J'], tree_tag_map['V'], tree_tag_map['R'] = wn.ADJ, wn.VERB, wn.ADV\n",
    "lemmatize_function = WordNetLemmatizer()\n",
    "lemmatized_words_dp, LEMMATIZE_DP_LIMIT = {}, 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lemmatize(token):\n",
    "    if lemmatized_words_dp.get(token):\n",
    "        return lemmatized_words_dp[token]\n",
    "    new_token = [lemmatize_function.lemmatize(token, tree_tag_map[tag[0]]) for token, tag in pos_tag([token])]\n",
    "    if len(lemmatized_words_dp) != LEMMATIZE_DP_LIMIT:\n",
    "        lemmatized_words_dp[token] = new_token[0]\n",
    "    return new_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a0d51-fbe6-4439-bea1-0479d4e1c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\".\", \"\") # don't remove fullstops\n",
    "#remove = remove.replace(\"-\", \"\") # don't remove hyphens\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9bb98-adf5-465d-9739-0f4628ab7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x,extend=False):\n",
    "    global corpus\n",
    "    x = str(x).lower()\n",
    "    x = replace_typical_misspell(x)\n",
    "    x = re.sub(\"@[A-Za-z0-9]+\",\" \", x) # remove user IDs\n",
    "    x = re.sub(\"(http://.*?\\s)|(http://.*)\",' ',str(x)) #remove http links in the text\n",
    "    x = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",' ',str(x)) # remove IP addresses\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    x = clean_numbers(x)\n",
    "    x = remove_emoji(x)\n",
    "    x = re.sub(pattern, \"\", x)\n",
    "    x = x.replace(\"”\",\"\").replace(\"’\",\"\")\n",
    "    x = x.split(\" \")\n",
    "    x = [do_lemmatize(w) for w in x]\n",
    "    x = \" \".join(x)\n",
    "    x =\" \".join(x.split())\n",
    "    if extend:\n",
    "        corpus.extend(x.split(\".\"))\n",
    "    x = x.replace(\".\",\" \")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c658dc-f417-4bdf-9f4f-454e8c0f590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"unclean/train.csv\")#.head(500)\n",
    "test = pd.read_csv(\"unclean/test.csv\")#.head(500)\n",
    "val = pd.read_csv(\"unclean/val.csv\")#.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab801f4b-56f8-47d6-9d64-3c659a8d9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"heading\"] = train[\"heading\"].progress_apply(lambda x: clean_text(x,True))\n",
    "train[\"body\"] = train[\"body\"].progress_apply(lambda x: clean_text(x,True))\n",
    "test[\"heading\"] = test[\"heading\"].progress_apply(lambda x: clean_text(x))\n",
    "test[\"body\"] = test[\"body\"].progress_apply(lambda x: clean_text(x))\n",
    "val[\"heading\"] = val[\"heading\"].progress_apply(lambda x: clean_text(x))\n",
    "val[\"body\"] = val[\"body\"].progress_apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e5380-06d6-4654-a62d-11fd126d9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"clean/train.csv\",index=False)\n",
    "test.to_csv(\"clean/test.csv\",index=False)\n",
    "val.to_csv(\"clean/val.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2def6ea-5360-4f9e-b216-af42343e88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa40f8d-a45e-47ce-aecb-29ea25365632",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932cf53-66f4-47e2-bc55-4542f7f0ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641e0f2-55b3-44d7-8dad-147153804d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./clean/corpus.json\",\"w\") as f:\n",
    "    json.dump(corpus,f,indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0549e4-9772-4d70-ac0b-13636a56f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42758162-f96c-46fd-a785-e5ba68b62c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
